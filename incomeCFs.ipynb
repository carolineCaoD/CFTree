{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import dice_ml\n",
    "from dice_ml.utils import helpers \n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "import json\n",
    "\n",
    "datasetKey = \"income\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = helpers.load_adult_income_dataset() \n",
    "\n",
    "print(df.info())  \n",
    "print(df.head())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in each column\n",
    "missing_values = df.isnull().sum()\n",
    "\n",
    "# Display only columns with missing values\n",
    "print(\"Columns with missing values:\\n\", missing_values[missing_values > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dice_ml.Data(dataframe=df,continuous_features=['age', 'hours_per_week'], outcome_name='income')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df[\"income\"]\n",
    "# Split data into train and test\n",
    "datasetX = df.drop(\"income\", axis=1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(datasetX,\n",
    "                                                    target,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=0,\n",
    "                                                    stratify=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see some examples\n",
    "young_female_condition = (df['gender'] == 'Female') & (df['age'] == 23) & (df['education'] == 'HS-grad') & (df['hours_per_week'] == 40) & (df['marital_status'] == 'Single') \n",
    "df[young_female_condition]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_first_20 = x_test[:20].copy()\n",
    "\n",
    "print(f\"Indices in dataset: {x_test_first_20.index.tolist()}\")\n",
    "\n",
    "x_test_indices = x_test_first_20.index\n",
    "x_test_first_20[\"target\"] = y_test.loc[x_test_indices].values\n",
    "\n",
    "x_test_first_20_with_target = x_test_first_20\n",
    "\n",
    "data_dict = {\"originalDataset\": x_test_first_20_with_target.to_dict(orient=\"records\")}\n",
    "\n",
    "json_filename = f\"{datasetKey}_x_test_with_target.json\"\n",
    "\n",
    "\n",
    "with open(json_filename, \"w\") as json_file:\n",
    "    json.dump(data_dict, json_file, indent=4)\n",
    "\n",
    "print(f\"Saved {len(x_test_first_20)} rows to '{json_filename}' including the target values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical = ['age', 'hours_per_week']\n",
    "categorical = x_train.columns.difference(numerical)\n",
    "\n",
    "# We create the preprocessing pipelines for both numeric and categorical data.\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "transformations = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical),\n",
    "        ('cat', categorical_transformer, categorical)])\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "clf = Pipeline(steps=[('preprocessor', transformations),\n",
    "                      ('classifier', RandomForestClassifier())])\n",
    "model = clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide the trained ML model to DiCE's model object\n",
    "backend = 'sklearn'\n",
    "m = dice_ml.Model(model=model, backend=backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate DiCE\n",
    "exp_random = dice_ml.Dice(d, m, method=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_first_20_with_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_first_20_features = x_test_first_20_with_target.drop('target', axis=1)\n",
    "\n",
    "dice_exp_random = exp_random.generate_counterfactuals(x_test_first_20_features, total_CFs=25, desired_class=\"opposite\", verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_exp_random.visualize_as_dataframe(show_only_changes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_pickle(dice_exp_object, datasetKey):\n",
    "\n",
    "    filename = f\"dice_exp_{datasetKey}.pkl\"  \n",
    "\n",
    "    with open(filename, \"wb\") as file:\n",
    "        pickle.dump(dice_exp_object, file)\n",
    "\n",
    "    print(f\"{datasetKey} counterfactuals have been pickled and saved as '{filename}'\")\n",
    "\n",
    "    return filename  \n",
    "\n",
    "save_pickle(dice_exp_random, datasetKey)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
